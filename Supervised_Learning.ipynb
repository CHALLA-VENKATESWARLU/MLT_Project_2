{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Supervised_Learning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNx8obQGlhBZkdSnXWt6gSg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"id":"syCJbUShu0RA","executionInfo":{"status":"ok","timestamp":1610037866657,"user_tz":-330,"elapsed":2317,"user":{"displayName":"Venkateswarlu Ch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgxCijRFIqPnkDEhQamQ2v7crsMP6Vr3QJSVsMXuA=s64","userId":"09197394552879099383"}},"outputId":"67158340-4cec-4cd6-c727-f6fda00eafd1"},"source":["import os\r\n","from pandas import read_csv, concat\r\n","\r\n","# Load data\r\n","data_path = os.path.join(os.getcwd(), \"titanic.csv\")\r\n","dataset = read_csv(data_path, skipinitialspace=True)\r\n","\r\n","dataset.head(5)"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n","0            1         0       3  ...   7.2500   NaN         S\n","1            2         1       1  ...  71.2833   C85         C\n","2            3         1       3  ...   7.9250   NaN         S\n","3            4         1       1  ...  53.1000  C123         S\n","4            5         0       3  ...   8.0500   NaN         S\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":607},"id":"abkxDrkMvMR2","executionInfo":{"status":"ok","timestamp":1610037917343,"user_tz":-330,"elapsed":1193,"user":{"displayName":"Venkateswarlu Ch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgxCijRFIqPnkDEhQamQ2v7crsMP6Vr3QJSVsMXuA=s64","userId":"09197394552879099383"}},"outputId":"5211a56c-bcf7-4c6c-edb8-f0cad9aa39b4"},"source":["import pandas as pd\r\n","\r\n","# We need to drop some insignificant features and map the others.\r\n","# Ticket number and fare should not contribute much to the performance of our models.\r\n","# Name feature has titles (e.g., Mr., Miss, Doctor) included.\r\n","# Gender is definitely important.\r\n","# Port of embarkation may contribute some value.\r\n","# Using port of embarkation may sound counter-intuitive; however, there may \r\n","# be a higher survival rate for passengers who boarded in the same port.\r\n","\r\n","dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\r\n","dataset = dataset.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'], axis=1)\r\n","\r\n","pd.crosstab(dataset['Title'], dataset['Sex'])"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Sex</th>\n","      <th>female</th>\n","      <th>male</th>\n","    </tr>\n","    <tr>\n","      <th>Title</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Capt</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>Col</th>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>Countess</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Don</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>Dr</th>\n","      <td>1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>Jonkheer</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>Lady</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Major</th>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>Master</th>\n","      <td>0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>Miss</th>\n","      <td>182</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Mlle</th>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Mme</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Mr</th>\n","      <td>0</td>\n","      <td>517</td>\n","    </tr>\n","    <tr>\n","      <th>Mrs</th>\n","      <td>125</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Ms</th>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Rev</th>\n","      <td>0</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>Sir</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Sex       female  male\n","Title                 \n","Capt           0     1\n","Col            0     2\n","Countess       1     0\n","Don            0     1\n","Dr             1     6\n","Jonkheer       0     1\n","Lady           1     0\n","Major          0     2\n","Master         0    40\n","Miss         182     0\n","Mlle           2     0\n","Mme            1     0\n","Mr             0   517\n","Mrs          125     0\n","Ms             1     0\n","Rev            0     6\n","Sir            0     1"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"lui-PIeCvUNV","executionInfo":{"status":"ok","timestamp":1610037943450,"user_tz":-330,"elapsed":1277,"user":{"displayName":"Venkateswarlu Ch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgxCijRFIqPnkDEhQamQ2v7crsMP6Vr3QJSVsMXuA=s64","userId":"09197394552879099383"}},"outputId":"3d90e5f8-4911-4e9f-fa3c-70f9923d1982"},"source":["# We will replace many titles with a more common name, English equivalent,\r\n","# or reclassification\r\n","dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\r\n"," \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\r\n","\r\n","dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\r\n","dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\r\n","dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\r\n","    \r\n","dataset[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Title</th>\n","      <th>Survived</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Master</td>\n","      <td>0.575000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Miss</td>\n","      <td>0.702703</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Mr</td>\n","      <td>0.156673</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Mrs</td>\n","      <td>0.793651</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Other</td>\n","      <td>0.347826</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Title  Survived\n","0  Master  0.575000\n","1    Miss  0.702703\n","2      Mr  0.156673\n","3     Mrs  0.793651\n","4   Other  0.347826"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"kvTyUaOdvYjP","executionInfo":{"status":"ok","timestamp":1610037984407,"user_tz":-330,"elapsed":1362,"user":{"displayName":"Venkateswarlu Ch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgxCijRFIqPnkDEhQamQ2v7crsMP6Vr3QJSVsMXuA=s64","userId":"09197394552879099383"}},"outputId":"9a063863-a4f7-48fb-f6fb-5fab9325734f"},"source":["# Now we will map alphanumerical categories to numbers\r\n","title_mapping = { 'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Other': 5 }\r\n","gender_mapping = { 'female': 1, 'male': 0 }\r\n","port_mapping = { 'S': 0, 'C': 1, 'Q': 2 }\r\n","\r\n","# Map title\r\n","dataset['Title'] = dataset['Title'].map(title_mapping).astype(int)\r\n","\r\n","# Map gender\r\n","dataset['Sex'] = dataset['Sex'].map(gender_mapping).astype(int)\r\n","\r\n","# Map port\r\n","freq_port = dataset.Embarked.dropna().mode()[0]\r\n","dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\r\n","dataset['Embarked'] = dataset['Embarked'].map(port_mapping).astype(int)\r\n","\r\n","# Fix missing age values\r\n","dataset['Age'] = dataset['Age'].fillna(dataset['Age'].dropna().median())\r\n","\r\n","dataset.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Fare</th>\n","      <th>Embarked</th>\n","      <th>Title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53.1000</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8.0500</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  Title\n","0         0       3    0  22.0      1      0   7.2500         0      1\n","1         1       1    1  38.0      1      0  71.2833         1      3\n","2         1       3    1  26.0      0      0   7.9250         0      2\n","3         1       1    1  35.0      1      0  53.1000         0      3\n","4         0       3    0  35.0      0      0   8.0500         0      1"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RSwha9-rxhjI","executionInfo":{"status":"ok","timestamp":1610039139710,"user_tz":-330,"elapsed":5822,"user":{"displayName":"Venkateswarlu Ch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgxCijRFIqPnkDEhQamQ2v7crsMP6Vr3QJSVsMXuA=s64","userId":"09197394552879099383"}},"outputId":"e083a79f-b238-49d5-f1fe-901519aa67bb"},"source":["from sklearn.model_selection import KFold, train_test_split\r\n","from sklearn.pipeline import make_pipeline\r\n","from sklearn.preprocessing import PolynomialFeatures, StandardScaler\r\n","\r\n","from sklearn.neural_network import MLPClassifier\r\n","from sklearn.svm import SVC\r\n","from sklearn.linear_model import LogisticRegression\r\n","\r\n","\r\n","# Prepare the data\r\n","X = dataset.drop(['Survived'], axis = 1).values\r\n","y = dataset[['Survived']].values\r\n","\r\n","X = StandardScaler().fit_transform(X)\r\n","\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = None)\r\n","\r\n","# Prepare cross-validation (cv)\r\n","cv = KFold(n_splits = 5, random_state = None)\r\n","\r\n","# Performance\r\n","p_score = lambda model, score: print('Performance of the %s model is %0.2f%%' % (model, score * 100))\r\n","\r\n","# Classifiers\r\n","names = [\r\n","    \"Logistic Regression\", \"Logistic Regression with Polynomial Hypotheses\",\r\n","    \"Linear SVM\", \"RBF SVM\", \"Neural Net\",\r\n","]\r\n","\r\n","classifiers = [\r\n","    LogisticRegression(),\r\n","    make_pipeline(PolynomialFeatures(3), LogisticRegression()),\r\n","    SVC(kernel=\"linear\", C=0.025),\r\n","    SVC(gamma=2, C=1),\r\n","    MLPClassifier(alpha=1),\r\n","]\r\n","# iterate over classifiers\r\n","models = []\r\n","trained_classifiers = []\r\n","for name, clf in zip(names, classifiers):\r\n","    scores = []\r\n","    for train_indices, test_indices in cv.split(X):\r\n","        clf.fit(X[train_indices], y[train_indices].ravel())\r\n","        scores.append( clf.score(X_test, y_test.ravel()) )\r\n","    \r\n","    min_score = min(scores)\r\n","    max_score = max(scores)\r\n","    avg_score = sum(scores) / len(scores)\r\n","    \r\n","    trained_classifiers.append(clf)\r\n","    models.append((name, min_score, max_score, avg_score))\r\n","    \r\n","fin_models = pd.DataFrame(models, columns = ['Name', 'Min Score', 'Max Score', 'Mean Score'])\r\n","fin_models.sort_values(['Mean Score']).head()\r\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Min Score</th>\n","      <th>Max Score</th>\n","      <th>Mean Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>Linear SVM</td>\n","      <td>0.776536</td>\n","      <td>0.787709</td>\n","      <td>0.781006</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Logistic Regression</td>\n","      <td>0.770950</td>\n","      <td>0.793296</td>\n","      <td>0.784358</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Neural Net</td>\n","      <td>0.815642</td>\n","      <td>0.832402</td>\n","      <td>0.823464</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Logistic Regression with Polynomial Hypotheses</td>\n","      <td>0.821229</td>\n","      <td>0.854749</td>\n","      <td>0.837989</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>RBF SVM</td>\n","      <td>0.837989</td>\n","      <td>0.899441</td>\n","      <td>0.864804</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             Name  ...  Mean Score\n","2                                      Linear SVM  ...    0.781006\n","0                             Logistic Regression  ...    0.784358\n","4                                      Neural Net  ...    0.823464\n","1  Logistic Regression with Polynomial Hypotheses  ...    0.837989\n","3                                         RBF SVM  ...    0.864804\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"s7DfjGs60jKr"},"source":["# New section"]}]}